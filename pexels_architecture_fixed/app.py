import streamlit as st
import torch
import clip
from PIL import Image
import numpy as np
from scipy.spatial.distance import cdist
import os
from pathlib import Path
import matplotlib.pyplot as plt


# ========== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ==========
st.set_page_config(
    page_title="–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –ø–æ–∏—Å–∫",
    page_icon="üèõÔ∏è",
    layout="wide"
)

EMBEDDINGS_FILE = 'clip_embeddings.npy'
PATHS_FILE = 'clip_image_paths.npy'
DATASET_PATH = 'pexels_architecture_fixed'
MODEL_NAME = 'ViT-B/32' 
TOP_K = 5

# ======== –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω–Ω—ã—Ö ========
@st.cache_resource
def load_clip_model():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model, preprocess = clip.load(MODEL_NAME, device=device)
    model.eval()
    return model, preprocess, device

@st.cache_data
def load_embeddings():
    if not os.path.exists(EMBEDDINGS_FILE):
        st.error(f'–§–∞–π–ª {EMBEDDINGS_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω !')
        st.info('–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö')
        return None, None
    
    embeddings = np.load(EMBEDDINGS_FILE)
    paths = np.load(PATHS_FILE, allow_pickle=True)
    return embeddings, paths


# ========== –°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥==========

def get_image_embedding(_model, _preprocess, _device, img_path_or_uploaded_fiile):
    """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        if hasattr(img_path_or_uploaded_fiile, 'read'):
            image = Image.open(img_path_or_uploaded_fiile).convert('RGB')
        else:
            image = Image.open(img_path_or_uploaded_fiile).convert('RGB')

        image_input = _preprocess(image).unsqueeze(0).to(_device)
        with torch.no_grad():
            embedding = _model.encode_image(image_input)
            embedding /= embedding.norm(dim=-1, keepdim=True)
        return embedding.cpu().numpy().flatten()
    except Exception as e:
        st.error(f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}')
        return None
    
def get_text_embedding(_model, _device, text_query):
    """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞"""
    try:
        text_input = clip.tokenize([f'a photo of {text_query}']).to(_device)
        with torch.no_grad():
            embedding = _model.encode_text(text_input)
            embedding /= embedding.norm(dim=-1, keepdim=True)
        return embedding.cpu().numpy().flatten()
    except Exception as e:
        st.error('–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: {e}')
        return None
    
# ============== –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö ================  
    
def search_by_image(query_img, _model, _preprocess, _device, embedding, paths, top_k=TOP_K):
    """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    query_emb = get_image_embedding(_model, _preprocess, _device, query_img)
    if query_emb is None:
        return []
    
    query_emb_2d = query_emb.reshape(1, -1)
    similarities = 1 - cdist(query_emb_2d, embedding, 'cosine')[0]
    top_indices = np.argsort(similarities)[::-1][:top_k]
    return [(paths[idx], similarities[idx]) for idx in top_indices]

def search_by_text(query_text, _model, _device, embeddings, paths, top_k = TOP_K):
    """–ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É"""
    query_emb = get_text_embedding(_model, _device, query_text)
    if query_emb is None:
        return []
    
    query_emb_2d = query_emb.reshape(1,-1)
    similarities = (embeddings @ query_emb_2d.T).flatten()
    top_indices = np.argsort(similarities)[::-1][:top_k]

    return [(paths[idx], similarities[idx]) for idx in top_indices]

#======================================================================

def zero_shot_classify(query_img, _model, _preprocess, _device, class_descriptions=None):
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è"""
    if class_descriptions is None:
        class_descriptions =[
            'modern skyscraper with glass fasade',
            'classical building with columns',
            'gothic cathedral with stained glass',
            'traditional wooden house',
            'brutalist concrete structure',
            'industrial warehouse with brick walls',
            'contemporary minimalist building',
            'art deco skyscraper',
            'medieval castle with towers',
            'modernist villa with clean lines'
        ]
    
    image_emb = get_image_embedding(_model, _preprocess, _device, query_img)
    if image_emb is None:
        return []
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    text_inputs = clip.tokenize(class_descriptions).to(_device)

    with torch.no_grad():
        text_embeddings = _model.encode_text(text_inputs)
        text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)

# –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ

        image_emb_2d = image_emb.reshape(1, -1)
        similarity = (100.0 * image_emb_2d @ text_embeddings.T)
        probs = similarity.softmax(dim=-1)

        values, indices = probs[0].topk(min(5, len(class_descriptions)))

    return [(class_descriptions[idx], val.item()) for val, idx in zip(values, indices)]


# ========== –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –°–û–ó–î–ê–ù–ò–Ø –ë–ê–ó–´ –î–ê–ù–ù–´–• ==========

def create_database(_model, _preprocess, _device):
    """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤—Ä—É—á–Ω—É—é)"""

    st.info('–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç.')
    all_paths = []
    for ext in ('*.jpg', '*.jpeg', '*.png'):
        all_paths.extend(Path(DATASET_PATH).rglob(ext))

    st.write(f'–ù–∞–π–¥–µ–Ω–æ {len(all_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π')

    embeddings = []
    valid_paths = []
    progress_bar = st.progress(0)

    for i, img_path in enumerate(all_paths):
        emb = get_image_embedding(_model, _preprocess, _device, str(img_path))
        if emb is not None:
            embeddings.append(emb)
            valid_paths.append(str(img_path))

            if (i+1) % 10 == 0 or (i+1) == len(all_paths):
                progress_bar.progress((i+1)/ len(all_paths))
        if embeddings:
            embeddings_array = np.vstack(embeddings)
            np.save(EMBEDDINGS_FILE, embeddings_array)
            np.save(PATHS_FILE, np.array(valid_paths))

            st.success(f'–ë–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞: {embeddings_array.shape[0]} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤')
            st.write(f'–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embeddings_array, np.array(valid_paths)}')
            return embeddings_array, np.array(valid_paths)
        else:
            st.error('–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏')
            return None, None
        
 # ========== –û–°–ù–û–í–ù–û–ô –ò–ù–¢–ï–†–§–ï–ô–° ==========

def main():
    st.title("üèõÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º CLIP")
    st.markdown("–°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

    model, preprocess, device = load_clip_model()
    embeddings, paths = load_embeddings()

    with st.sidebar:
        st.header('‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏')


        mode = st.radio(
            "–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:",
            ["üîç –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É", "üñºÔ∏è –ü–æ–∏—Å–∫ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é", "üè∑Ô∏è –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—Ç–∏–ª—è", "üóÉÔ∏è –°–æ–∑–¥–∞—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"]
        )

        top_k = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:", 1, 20, TOP_K)

        st.divider()
        st.header("‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
        if embeddings is not None:
            st.write(f"**–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö:** {len(paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            st.write(f"**–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:** {embeddings.shape[1]}")
        st.write(f"**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:** {device}")
        st.write(f"**–ú–æ–¥–µ–ª—å:** CLIP ViT-B/32")

    # –û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
    if mode == "üîç –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É":
        st.header("–ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é")
        
        # –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤
        col1, col2, col3 = st.columns(3)
        with col1:
            if st.button("modern glass skyscraper"):
                st.session_state.text_query = "modern glass skyscraper"
        with col2:
            if st.button("classical building with columns"):
                st.session_state.text_query = "classical building with columns"
        with col3:
            if st.button("gothic cathedral"):
                st.session_state.text_query = "gothic cathedral"
        
        # –ü–æ–ª–µ –¥–ª—è –≤–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞
        text_query = st.text_input(
            "–í–≤–µ–¥–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:",
            value=getattr(st.session_state, 'text_query', 'modern building'),
            placeholder="–Ω–∞–ø—Ä–∏–º–µ—Ä: modern glass skyscraper"
        )
        
        if st.button("–ù–∞–π—Ç–∏", type="primary") or text_query:
            if embeddings is None:
                st.error("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
                return
            
            with st.spinner("–ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è..."):
                results = search_by_text(text_query, model, device, embeddings, paths, top_k)
            
            if results:
                st.success(f"–ù–∞–π–¥–µ–Ω–æ {len(results)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                
                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Å–µ—Ç–∫–µ
                cols = st.columns(min(4, len(results)))
                for idx, (col, (path, score)) in enumerate(zip(cols, results)):
                    with col:
                        try:
                            img = Image.open(path).convert('RGB')
                            st.image(img, use_container_width=True)
                            st.caption(f"**–°—Ö–æ–¥—Å—Ç–≤–æ:** {score:.3f}")
                            st.caption(f"**–§–∞–π–ª:** {os.path.basename(path)}")
                            
                            # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
                            if st.button(f"üîç –ü–æ–¥—Ä–æ–±–Ω–µ–µ {idx+1}", key=f"detail_{idx}"):
                                st.session_state.selected_image = path
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
                
                # –î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                if hasattr(st.session_state, 'selected_image'):
                    st.divider()
                    st.subheader("üì∏ –î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä")
                    col1, col2 = st.columns([1, 2])
                    with col1:
                        st.image(st.session_state.selected_image, use_container_width=True)
                    with col2:
                        st.write(f"**–ü–æ–ª–Ω—ã–π –ø—É—Ç—å:** {st.session_state.selected_image}")
                        st.write(f"**–†–∞–∑–º–µ—Ä:** {Image.open(st.session_state.selected_image).size}")
            else:
                st.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    elif mode == "üñºÔ∏è –ü–æ–∏—Å–∫ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é":
        st.header("–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        uploaded_file = st.file_uploader(
            "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞",
            type=['jpg', 'jpeg', 'png'],
            help="–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"
        )
        
        # –ü—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        st.write("–ò–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–º–µ—Ä –∏–∑ –±–∞–∑—ã:")
        if paths is not None and len(paths) > 0:
            sample_cols = st.columns(4)
            sample_indices = np.random.choice(len(paths), 4, replace=False)
            for col, idx in zip(sample_cols, sample_indices):
                with col:
                    if st.button(f"–ü—Ä–∏–º–µ—Ä {idx+1}", key=f"sample_{idx}"):
                        st.session_state.sample_image = paths[idx]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–ª–∏ –≤—ã–±—Ä–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        query_image = uploaded_file if uploaded_file else getattr(st.session_state, 'sample_image', None)
        
        if query_image:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
            col1, col2 = st.columns([1, 2])
            with col1:
                if hasattr(query_image, 'read'):
                    img = Image.open(query_image).convert('RGB')
                    st.image(img, caption="–í–∞—à –∑–∞–ø—Ä–æ—Å", use_container_width=True)
                else:
                    st.image(query_image, caption="–í–∞—à –∑–∞–ø—Ä–æ—Å", use_container_width=True)
            
            with col2:
                st.write("**–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–ø—Ä–æ—Å–µ:**")
                if hasattr(query_image, 'name'):
                    st.write(f"**–ò–º—è —Ñ–∞–π–ª–∞:** {query_image.name}")
                    st.write(f"**–¢–∏–ø:** {query_image.type}")
                else:
                    st.write(f"**–ü—É—Ç—å:** {query_image}")
            
            if st.button("–ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ", type="primary") and embeddings is not None:
                with st.spinner("–ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è..."):
                    results = search_by_image(query_image, model, preprocess, device, embeddings, paths, top_k)
                
                if results:
                    st.subheader(f"üéØ –¢–æ–ø-{len(results)} –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:")
                    
                    # –°–µ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    for i in range(0, len(results), 4):
                        cols = st.columns(4)
                        for col_idx in range(4):
                            if i + col_idx < len(results):
                                path, score = results[i + col_idx]
                                with cols[col_idx]:
                                    try:
                                        img = Image.open(path).convert('RGB')
                                        st.image(img, use_container_width=True)
                                        
                                        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞
                                        st.progress(float(score))
                                        
                                        st.caption(f"**–°—Ö–æ–¥—Å—Ç–≤–æ:** {score:.3f}")
                                        st.caption(f"**{os.path.basename(path)}**")
                                    except Exception as e:
                                        st.error(f"–û—à–∏–±–∫–∞: {e}")
    
    elif mode == "üè∑Ô∏è –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—Ç–∏–ª—è":
        st.header("–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–≥–æ —Å—Ç–∏–ª—è")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        uploaded_file = st.file_uploader(
            "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏",
            type=['jpg', 'jpeg', 'png']
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∞—Å—Å–æ–≤
        st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
        
        default_classes = [
            "modern skyscraper with glass facade",
            "classical building with columns",
            "gothic cathedral with stained glass",
            "traditional wooden house",
            "brutalist concrete structure"
        ]
        
        custom_classes = st.text_area(
            "–í–≤–µ–¥–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ (–∫–∞–∂–¥–æ–µ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏):",
            value="\n".join(default_classes),
            height=150,
            help="–ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ - –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å. –ù–∞–ø—Ä–∏–º–µ—Ä: 'modern glass building'"
        )
        
        class_list = [c.strip() for c in custom_classes.split('\n') if c.strip()]
        
        if uploaded_file and class_list:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            col1, col2 = st.columns([1, 2])
            with col1:
                img = Image.open(uploaded_file).convert('RGB')
                st.image(img, caption="–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏", use_container_width=True)
            
            if st.button("–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å", type="primary"):
                with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∏–ª—å..."):
                    results = zero_shot_classify(uploaded_file, model, preprocess, device, class_list)
                
                if results:
                    st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
                    
                    # –°—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
                    classes = [r[0] for r in results]
                    scores = [r[1] for r in results]
                    
                    fig, ax = plt.subplots(figsize=(10, 6))
                    bars = ax.barh(classes, scores, color='skyblue')
                    ax.set_xlabel('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
                    ax.set_title('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —Å—Ç–∏–ª–µ–π')
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –≥—Ä–∞—Ñ–∏–∫
                    for bar, score in zip(bars, scores):
                        width = bar.get_width()
                        ax.text(width + 0.01, bar.get_y() + bar.get_height()/2,
                               f'{score:.2%}', va='center')
                    
                    st.pyplot(fig)
                    
                    # –¢–∞–±–ª–∏—Ü–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                    st.subheader("üìã –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
                    for i, (class_name, prob) in enumerate(results, 1):
                        st.write(f"{i}. **{class_name}** ‚Üí {prob:.2%}")
                        
                        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
                        st.progress(float(prob))
    
    elif mode == "üóÉÔ∏è –°–æ–∑–¥–∞—Ç—å –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö":
        st.header("–°–æ–∑–¥–∞–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
        
        st.warning("‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –≠—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ –ø–µ—Ä–µ–∑–∞–ø–∏—à–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!")
        
        if st.button("–°–æ–∑–¥–∞—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö", type="primary"):
            if not os.path.exists(DATASET_PATH):
                st.error(f"–ü–∞–ø–∫–∞ {DATASET_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
                return
            
            embeddings_new, paths_new = create_database(model, preprocess, device)
            if embeddings_new is not None:
                st.success("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!")
                st.rerun()  

# ========== –ó–ê–ü–£–°–ö ==========
if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    try:
        import streamlit
        import torch
        import clip
        main()
    except ImportError as e:
        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥—É–ª—å: {e}")
       


